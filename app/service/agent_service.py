import os
import json
from typing import List, Dict, Any, Optional
from openai import OpenAI
from dotenv import load_dotenv

from app.service.vector_service import VectorService
from app.service.time_service import TimeService

load_dotenv()

class AgentService:
    def __init__(self, vector_service: VectorService, time_service: TimeService):
        api_key = os.getenv("UPSTAGE_API_KEY")
        if not api_key:
            raise ValueError("UPSTAGE_API_KEY environment variable is required")

        # Upstage API Client ì„¤ì •
        self.client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1")
        self.vector_service = vector_service
        self.time_service = time_service
        self.model_name = "solar-1-mini-chat"

        # [Function Calling] LLMì´ ì‚¬ìš©í•  ë„êµ¬ ì •ì˜
        self.tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_current_time",
                    "description": "Get the current real-time for a specific timezone. Use this when user asks for 'now', 'current time'.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "timezone": {
                                "type": "string",
                                "description": "The IANA timezone ID, e.g., 'Asia/Seoul', 'America/New_York', 'Europe/London'.",
                            }
                        },
                        "required": ["timezone"],
                    },
                },
            }
        ]

    def process_query(self, query: str, context_limit: int = 3) -> Dict[str, Any]:
        # Step 1: Retrieve relevant documents using vector search
        search_results = self.vector_service.search(query, n_results=context_limit)

        # Step 2: Prepare context from retrieved documents
        context = self._prepare_context(search_results)

        # Step 3: Generate response using Upstage Solar LLM
        response = self._generate_response(query, context)

        return {
            "query": query,
            "response": response,
            "retrieved_documents": search_results["documents"],
            "document_distances": search_results["distances"],
            "context_used": context
        }

    def _prepare_context(self, search_results: Dict[str, Any]) -> str:
        """
        [ìˆ˜ì •ë¨] rules.json êµ¬ì¡°ì— ë§ì¶° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ë…ì„± ìˆëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        documents = search_results.get("documents", [])
        metadatas = search_results.get("metadatas", [])

        if not documents:
            return "No relevant internal regulations found."

        context_parts = []
        for i, doc in enumerate(documents):
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (DBì— ì €ì¥ë  ë•Œ office_name ë“±ì´ ë©”íƒ€ë°ì´í„°ë¡œ ë“¤ì–´ê°”ë‹¤ê³  ê°€ì •)
            meta = metadatas[i] if metadatas and i < len(metadatas) else {}

            # rules.jsonì˜ íŠ¹ì„±ì„ ë°˜ì˜í•œ í¬ë§·íŒ…
            office_name = meta.get("office_name", "Unknown Office")
            timezone = meta.get("timezone", "Unknown Timezone")
            country = meta.get("country", "")

            # ë¬¸ë§¥ ì¡°ë¦½
            context_part = (
                f"[Source {i + 1}: {office_name} ({country})]\n"
                f"Timezone: {timezone}\n"
                f"Rule Description: {doc}\n"
            )
            context_parts.append(context_part)

        return "\n".join(context_parts)

    def _generate_response(self, query: str, context: str) -> str:
        # rules.json ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ System Prompt ê°•í™”
        system_prompt = (
            "You are a smart AI assistant for a global company. "
            "Use the provided Context to answer questions. "
            "IMPORTANT: If the user asks about availability, office hours, or contact (e.g., 'Can I call?'), "
            "you MUST use the 'get_current_time' tool to get the real-time of that specific timezone. "
            "Do not guess the time. Check it using the tool."
        )

        user_prompt = f"""Context:
{context}

Question: {query}

Please provide a helpful response based on the context above."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        try:
            response = self.client.chat.completions.create(
                model="solar-1-mini-chat",
                messages=messages,
                tools=self.tools,
                tool_choice="auto",
                temperature=0.3,
                max_tokens=500
            )

            response_message = response.choices[0].message
            tool_calls = response_message.tool_calls

            # [ë””ë²„ê¹…] ì‹¤ì œ ë„êµ¬ í˜¸ì¶œì´ ì¡í˜”ëŠ”ì§€ ì„œë²„ ë¡œê·¸ë¡œ í™•ì¸
            print(f"ğŸ¤– AI Response Content: {response_message.content}")
            print(f"ğŸ”§ Tool Calls Detected: {tool_calls}")

            # 2. ë„êµ¬ ì‹¤í–‰ì´ í•„ìš”í•œ ê²½ìš°
            if tool_calls:
                # ëŒ€í™” ë‚´ì—­ì— "ë‚˜ ë„êµ¬ ì“¸ê²Œ"ë¼ëŠ” AIì˜ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
                messages.append(response_message)

                for tool_call in tool_calls:
                    if tool_call.function.name == "get_current_time":
                        args = json.loads(tool_call.function.arguments)
                        timezone = args.get("timezone")

                        print(f"â° Checking time for: {timezone}")

                        # ì‹¤ì œ í•¨ìˆ˜ ì‹¤í–‰ (TimeService)
                        tool_result = self.time_service.get_current_time(timezone)

                        print(f"tool result: {tool_result}")

                        # ê²°ê³¼ ëŒ€í™” ë‚´ì—­ì— ì¶”ê°€
                        messages.append({
                            "tool_call_id": tool_call.id,
                            "role": "tool",
                            "name": "get_current_time",
                            "content": json.dumps(tool_result)  # ë°˜ë“œì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜
                        })

                # 3. ë„êµ¬ ê²°ê³¼ë¥¼ í¬í•¨í•´ì„œ ìµœì¢… ë‹µë³€ ìƒì„±
                final_response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=0.1
                )

                final_content = final_response.choices[0].message.content

                return final_content if final_content else "ì£„ì†¡í•©ë‹ˆë‹¤. ì‹œê°„ì„ í™•ì¸í–ˆìœ¼ë‚˜ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            # ë„êµ¬ë¥¼ ì•ˆ ì“´ ê²½ìš°
            content = response_message.content
            return content if content else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        except Exception as e:
            print(f"âŒ Error in _generate_response: {e}")
            return f"Error during generation: {str(e)}"


    def add_knowledge(self, documents: List[str], metadatas: List[Dict[str, Any]] = None):
        try:
            self.vector_service.add_documents(documents, metadatas)
            return {"status": "success", "message": f"Added {len(documents)} documents to knowledge base"}
        except Exception as e:
            return {"status": "error", "message": f"Failed to add documents: {str(e)}"}

    def get_knowledge_stats(self):
        return self.vector_service.get_collection_info()